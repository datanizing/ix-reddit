{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "03-Wortspiel.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqNTDexD51Ns"
      },
      "source": [
        "# iX-Artikel \"Beziehungssache\"\n",
        "von Stefanie Scholz und Christian Winkler\n",
        "\n",
        "## Voraussetzungen\n",
        "Leider können wir nicht alle einzelnen Verarbeitungsschritte darstellen, die für die Erzeugung der Grafik im Notebook notwendig sind, da dazu die gesamte Datenmenge des Subreddits benötigt wird."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca11Hclc51Nv"
      },
      "source": [
        "## Topic Models\n",
        "\n",
        "Die Gesamtdaten für die Topic Models sind zu groß und können nicht als Download bereitgestellt werden. Stattdessen wird hier im Notebook ein Topic Model für die Titel der Toplevel-Posts berechnet.\n",
        "\n",
        "### Achtung: dadurch ergeben sich andere Ergebnisse als im Artikel!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUNPbH_G51Nw"
      },
      "source": [
        "import pandas as pd\n",
        "all_posts = pd.read_csv(\"https://github.com/datanizing/ix-reddit/raw/main/all-toplevel-posts.csv.gz\", parse_dates=[\"created_utc\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzhUax7y51Nx"
      },
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS as stop_words\n",
        "\n",
        "# Stoppworte um Reddit-\"Slang\" ergänzen\n",
        "for w in \"amp at blog body buy buycheap call\\\n",
        "            can case change cheap co com could\\\n",
        "            create delete download drive email first fix\\\n",
        "            fuck go good help how http https\\\n",
        "            just late look make market message more\\\n",
        "            need new news now number online oral\\\n",
        "            page pass post question reddit remove review\\\n",
        "            say search self send should site support\\\n",
        "            test text time top unlock use video\\\n",
        "            watch way why will work\".split(\" \"):\n",
        "    stop_words.add(w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "0-qLY-T-51Nz"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(ngram_range=(1,2), max_df=0.7, min_df=5, max_features=10000, stop_words=stop_words)\n",
        "tfidf_vectors = tfidf.fit_transform(all_posts[\"title\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s359ENQV51Nz"
      },
      "source": [
        "from sklearn.decomposition import NMF\n",
        "\n",
        "num_topics = 20\n",
        "\n",
        "nmf = NMF(n_components = num_topics)\n",
        "nmf.fit(tfidf_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W31_-eEC51N1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "def wordcloud_topic_model_summary(model, feature_names, no_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        freq = {}\n",
        "        for i in topic.argsort()[:-no_top_words - 1:-1]:\n",
        "            val = int(100000.0 * topic[i])\n",
        "            freq[feature_names[i].replace(\" \", \"_\")] = val+1\n",
        "        wc = WordCloud(background_color=\"white\", max_words=100, width=960, height=540)\n",
        "        wc.generate_from_frequencies(freq)\n",
        "        plt.figure(figsize=(12,12))\n",
        "        plt.imshow(wc, interpolation='bilinear')\n",
        "        plt.axis(\"off\");\n",
        "            \n",
        "def display_topics(model, feature_names, no_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        first_index = topic.argsort()[-1]\n",
        "        print(\"Topic %s (%02d):\" % (feature_names[first_index], topic_idx))\n",
        "        print(\" \".join([\"'\"+feature_names[i]+\"'\"\n",
        "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMjDf_op51N2"
      },
      "source": [
        "def topics_table(model, feature_names, n_top_words = 20):\n",
        "    \n",
        "    # Aufbau eines DataFrames für die Anzeige\n",
        "    word_dict = {}\n",
        "    num_topics = model.n_components\n",
        "    \n",
        "    for i in range(num_topics):\n",
        "        \n",
        "        # ermittle für jedes Topic die größten Werte\n",
        "        # und füge die entsprechenden Worte im Klartext dem Dictionary hinzu\n",
        "        words_ids = model.components_[i].argsort()[:-n_top_words-1:-1]\n",
        "        words = [feature_names[key] for key in words_ids]\n",
        "        word_dict['Topic #%2d' % i] = words;\n",
        "    \n",
        "    display(pd.DataFrame(word_dict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ADRI6o1v51N3"
      },
      "source": [
        "wordcloud_topic_model_summary(nmf, tfidf.get_feature_names(), 40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DXt_sJh51N4"
      },
      "source": [
        "topics_table(nmf, tfidf.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSSqt0UD51N5"
      },
      "source": [
        "all_posts[\"month\"] = all_posts[\"created_utc\"].dt.strftime(\"%Y-%m\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2vHjKOc51N5"
      },
      "source": [
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "month_data = []\n",
        "for month in tqdm(np.unique(np.unique(all_posts[\"month\"]))):\n",
        "    W_month = nmf.transform(tfidf_vectors[np.array(all_posts[\"month\"] == month)])\n",
        "    month_data.append([month] + list(W_month.sum(axis=0)/W_month.sum()*100.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVa-64Bt51N6"
      },
      "source": [
        "topic_names = []\n",
        "voc = tfidf.get_feature_names()\n",
        "for topic in nmf.components_:\n",
        "    important = topic.argsort()\n",
        "    top_word = voc[important[-1]] + \" \" + voc[important[-2]]\n",
        "    topic_names.append(\"Topic \" + top_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Y7gHIqs51N6"
      },
      "source": [
        "df_month = pd.DataFrame(month_data, columns=[\"month\"] + topic_names).set_index(\"month\")\n",
        "df_month.plot.area(figsize=(16,9))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0odJPnHF51N7"
      },
      "source": [
        "## Klassifikation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM4PMIeU51N8"
      },
      "source": [
        "Zunächst wird eine Menge von positiven und negativen Samples benötigt. Im originalen Python-Code sieht das so aus:\n",
        "\n",
        "```python\n",
        "pos = pd.read_sql(\"SELECT created_utc, nav AS title FROM toplevel_posts2020 p, nlp_posts np\\\n",
        "                   WHERE np.id=p.id AND (flair='AI' OR flair='Artificial Intelligence') AND \\\n",
        "                         created_utc>='2015-05-01'\", sql, parse_dates=[\"created_utc\"])\n",
        "pos[\"target\"] = 1\n",
        "\n",
        "neg = pd.read_sql(\"SELECT created_utc, nav AS title FROM toplevel_posts2020 p, nlp_posts np\\\n",
        "                   WHERE np.id=p.id AND flair!='AI' AND flair!='Artificial Intelligence' AND flair IS NOT NULL AND \\\n",
        "                         created_utc>='2015-05-01'\", sql, parse_dates=[\"created_utc\"])\n",
        "neg[\"target\"] = 0\n",
        "\n",
        "data = pd.concat([pos, neg.sample(n = len(pos), random_state=42)], \n",
        "                 ignore_index=True)\n",
        "```\n",
        "\n",
        "Hier laden wir stattdessen den `DataFrame` direkt ein:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxT5lFFD51N9"
      },
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(\"https://github.com/datanizing/ix-reddit/raw/main/classification-data.csv.gz\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck-xlTfm51N-"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(ngram_range=(1,2), max_df=0.7, min_df=5, stop_words=stop_words)\n",
        "count_vectors = cv.fit_transform(data[\"title\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p9Tz74Q51N_"
      },
      "source": [
        "count_vectors.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxb8uubJ51OA"
      },
      "source": [
        "TF/IDF-Vektoren berechnen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtgffLud51OB"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf = TfidfTransformer(use_idf=True)\n",
        "tfidf_vectors = tfidf.fit_transform(count_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGesrDDA51OB"
      },
      "source": [
        "X = tfidf_vectors\n",
        "Y = data[\"target\"].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlgU_bVg51OC"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "clf = SGDClassifier(loss='hinge', max_iter=1000, tol=1e-3, random_state=42)\n",
        "clf.fit(X, Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEOnO63a51OC"
      },
      "source": [
        "Y_predicted = clf.predict(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkVqKEAk51OD"
      },
      "source": [
        "from sklearn import metrics\n",
        "metrics.accuracy_score(Y, Y_predicted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9X3ZPUC51OD"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "conf_mat = confusion_matrix(Y, Y_predicted)\n",
        "conf_mat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXEkKafu51OE"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 4))\n",
        "category_names = [\"negative\", \"positive\"]\n",
        "sns.heatmap(conf_mat, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
        "            xticklabels=category_names, yticklabels=category_names)\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.xlabel(\"Predicted\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oFJhGej51OE"
      },
      "source": [
        "### Hold-out-Verfahren: Getrennte Mengen für Training und Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-09-18T20:08:34.239506Z",
          "start_time": "2018-09-18T20:08:34.227489Z"
        },
        "id": "I6Pvl1aS51OF"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-09-18T20:08:37.830488Z",
          "start_time": "2018-09-18T20:08:37.822490Z"
        },
        "id": "YYJS5rnV51OF"
      },
      "source": [
        "X.shape\n",
        "\n",
        "X_train.shape\n",
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j26ADnhL51OG"
      },
      "source": [
        "Modell nur mit Trainingsdaten trainieren."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_ax4BXv51OG"
      },
      "source": [
        "clf.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hQsQX4y51OH"
      },
      "source": [
        "Ermittlung der Performance auf den Trainingsdaten selbst."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-09-18T19:22:16.336395Z",
          "start_time": "2018-09-18T19:22:16.310750Z"
        },
        "id": "tNF-qs2i51OH"
      },
      "source": [
        "Y_predicted = clf.predict(X_train)\n",
        "\n",
        "metrics.accuracy_score(Y_train, Y_predicted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc9ZFnvU51OI"
      },
      "source": [
        "Ermittlung der Performance auf den Testdaten."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-09-18T19:22:16.336395Z",
          "start_time": "2018-09-18T19:22:16.310750Z"
        },
        "id": "bel570SE51OI"
      },
      "source": [
        "Y_predicted = clf.predict(X_test)\n",
        "\n",
        "metrics.accuracy_score(Y_test, Y_predicted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amhs_SvG51OJ"
      },
      "source": [
        "print(metrics.classification_report(Y_test, Y_predicted, target_names=category_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Tj8KCI851OJ"
      },
      "source": [
        "## Stichproben"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG7_eaaR51OJ"
      },
      "source": [
        "Y_pred_all = clf.predict(tfidf.transform(cv.transform(data[\"title\"])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiC-aJeE51OK"
      },
      "source": [
        "data[\"pred\"] = Y_pred_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "RMbJclld51OK"
      },
      "source": [
        "data[data[\"pred\"] != data[\"target\"]][[\"title\", \"target\", \"pred\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwmU2R7c51OL"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEdHdg2R51OL"
      },
      "source": [
        "all_posts[\"ai\"] = clf.predict(tfidf.transform(cv.transform(all_posts[\"title\"])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f860qDU451OL"
      },
      "source": [
        "all_posts[\"ai\"].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1KY1MCe51OL"
      },
      "source": [
        "all_posts_m = all_posts.dropna(subset=[\"created_utc\"]).set_index(\"created_utc\").resample(\"M\").agg({ \"ai\": \"sum\", \"title\": \"count\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Czle6IW751OM"
      },
      "source": [
        "all_posts_m[\"rel\"] = all_posts_m[\"ai\"] / all_posts_m[\"title\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AUv18yX51OM"
      },
      "source": [
        "all_posts_m[[\"rel\"]].plot(figsize=(16,9))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qQmpGgw51OM"
      },
      "source": [
        "# Trend-Vorhersage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKDi7UP351OM"
      },
      "source": [
        "Ursprünglich erfolgte die Selektion mit folgendem Befehl:\n",
        "\n",
        "```python\n",
        "df = pd.read_sql(\"SELECT STRFTIME('%Y-%m-01', created_utc) AS month, flair, COUNT(*) AS count \\\n",
        "                  FROM toplevel_posts2020 \\\n",
        "                  WHERE created_utc>='2014-01-01' AND flair IN (SELECT flair FROM flairs WHERE count>1000) \\\n",
        "                  GROUP BY flair, month\", sql, parse_dates=[\"month\"])\n",
        "```\n",
        "\n",
        "Weil auch hier die Datenbasis zu groß ist, laden wir den `DataFrame` direkt ein:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNVU25OU51ON"
      },
      "source": [
        "df = pd.read_csv(\"https://github.com/datanizing/ix-reddit/raw/main/flairs-per-month.csv.gz\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxpiIY2051ON"
      },
      "source": [
        "past = df.pivot(index=\"flair\", columns=\"month\", values=\"count\").fillna(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8jaBV-h51ON"
      },
      "source": [
        "past"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3Szayb25-Py"
      },
      "source": [
        "!pip install prophet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnuv2i3o51ON"
      },
      "source": [
        "from prophet import Prophet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaeNuNG551OO"
      },
      "source": [
        "pa = pd.DataFrame()\n",
        "pa[\"ds\"] = past.columns\n",
        "pa[\"y\"] = past.loc[\"Business\"].values\n",
        "pa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOMqcmxd51OO"
      },
      "source": [
        "m = Prophet()\n",
        "m.fit(pa)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6VaWV-451OP"
      },
      "source": [
        "future = m.make_future_dataframe(periods=20, freq='M')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO_7mzg351OP"
      },
      "source": [
        "forecast = m.predict(future)\n",
        "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4GxI_J351OP"
      },
      "source": [
        "fig1 = m.plot(forecast)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXwfyb5_51OP"
      },
      "source": [
        "fig2 = m.plot_components(forecast)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}